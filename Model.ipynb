{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNXZeypBZ73Cn8HvR+15S01",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hazzd12/CASA0018_coursework/blob/main/Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Audio preprocessing\n",
        "First, we need to convert the original audio file to the Mayer spectrum, a common representation of audio features that is particularly suitable for feeding convolutional neural networks (CNNS) for training."
      ],
      "metadata": {
        "id": "MdY8G9PvjSUk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "kSJ7DXfcjDf1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import librosa\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from skimage.transform import resize\n",
        "\n",
        "def load_and_segment_audio(audio_path, target_length=1.5):\n",
        "    y, sr = librosa.load(audio_path)\n",
        "    buffer_length = int(sr * target_length)\n",
        "    segments = [y[i:i + buffer_length] for i in range(0, len(y), buffer_length) if i + buffer_length <= len(y)]\n",
        "    return segments, sr\n",
        "\n",
        "\n",
        "\n",
        "def add__noise(data_segment, noise_level=0.005):\n",
        "    # Ensure the noise is generated with the same shape as the data segment\n",
        "    noise = np.random.randn(*data_segment.shape)\n",
        "    augmented_data_segment = data_segment + noise_level * noise\n",
        "    return augmented_data_segment\n",
        "\n",
        "\n",
        "def resize_melspectrogram(mels, target_shape=(128, 128)):\n",
        "\n",
        "    return resize(mels, target_shape, mode='constant', anti_aliasing=True)\n",
        "\n",
        "def extract_melspectrogram(y, sr, n_fft=2048, hop_length=512, n_mels=128):\n",
        "    S = librosa.feature.melspectrogram(y=y, sr=sr, n_fft=n_fft, hop_length=hop_length, n_mels=n_mels)\n",
        "    S_DB = librosa.power_to_db(S, ref=np.max)\n",
        "    S_resized = resize_melspectrogram(S_DB, target_shape=(128, 128))\n",
        "    S_resized = S_resized[..., np.newaxis]\n",
        "    return S_resized\n"
      ],
      "metadata": {
        "id": "ulq-2cmvirdn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "def unzip_audio_files(zip_path, extract_path):\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_path)\n",
        "    print(f\"Extracted audio files to {extract_path}\")\n",
        "\n",
        "def delete_directory(directory_path):\n",
        "    try:\n",
        "        shutil.rmtree(directory_path)\n",
        "        print(f\"Directory '{directory_path}' deleted successfully.\")\n",
        "    except OSError as e:\n",
        "        print(f\"Error: {directory_path} : {e.strerror}\")\n",
        "\n",
        "zip_path = '/content/dataset/Data.zip'\n",
        "extract_path = '/content/dataset/data'\n",
        "\n",
        "delete_directory(extract_path)\n",
        "unzip_audio_files(zip_path, extract_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Lye6RTL3HKP",
        "outputId": "2c0f83eb-aeef-415b-9825-d11fe12a4636"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Directory '/content/dataset/data' deleted successfully.\n",
            "Extracted audio files to /content/dataset/data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def process_and_visualize(audio_path, target_length=1.5, noise_level=0.005):\n",
        "    # Load and segment audio\n",
        "    segments, sr = load_and_segment_audio(audio_path, target_length=target_length)\n",
        "\n",
        "    processed_segments = []\n",
        "    for segment in segments:\n",
        "        # Add noise to the individual segment\n",
        "        noisy_segment = add__noise(segment, noise_level=noise_level)\n",
        "\n",
        "        # Extract mel spectrogram\n",
        "        melspectrogram = extract_melspectrogram(noisy_segment, sr)\n",
        "\n",
        "        processed_segments.append(melspectrogram)\n",
        "\n",
        "    # If needed, visualize or further process the segments\n",
        "    return processed_segments\n",
        "\n",
        "\n",
        "#melspectrogram = process_and_visualize(str(audio_file))\n"
      ],
      "metadata": {
        "id": "ICbizXbZYyhg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "def resize_melspectrogram(mels, target_shape=(128, 128)):\n",
        "    return resize(mels, target_shape, mode='constant', anti_aliasing=True)\n",
        "\n",
        "def load_data_and_labels(audio_dir):\n",
        "    categories = [f.name for f in os.scandir(audio_dir) if f.is_dir()]\n",
        "    labels_dict = {category: i for i, category in enumerate(categories)}\n",
        "    print(labels_dict)\n",
        "    X, y = [], []\n",
        "    for category, label in labels_dict.items():\n",
        "        category_dir = Path(audio_dir) / category\n",
        "        for audio_file in category_dir.glob('*.ogg'):\n",
        "            try:\n",
        "                segments, sr = load_and_segment_audio(str(audio_file))\n",
        "                for segment in segments:\n",
        "                    segment = add__noise(segment)\n",
        "                    spectrogram = extract_melspectrogram(segment, sr)\n",
        "                    X.append(spectrogram)\n",
        "                    y.append(label)\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {audio_file}: {e}\")\n",
        "    return np.array(X), np.array(y), categories\n",
        "\n",
        "X, y, categories = load_data_and_labels(extract_path+'/Data')\n",
        "np.save('X.npy', X)\n",
        "np.save('y.npy', y)\n",
        "print(categories)"
      ],
      "metadata": {
        "id": "RyFfKdoAtLq7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cfd128a8-aeae-41a2-d54e-35036d37f051"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'102 - Rooster': 0, '105 - Frog': 1, '101 - Dog': 2, '104 - Cow': 3, '103 - Pig': 4}\n",
            "['102 - Rooster', '105 - Frog', '101 - Dog', '104 - Cow', '103 - Pig']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.regularizers import l2\n",
        "def build_model(input_shape, num_classes):\n",
        "    model = Sequential([\n",
        "        Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),\n",
        "        MaxPooling2D((2, 2)),\n",
        "        Conv2D(64, (3, 3), activation='relu'),\n",
        "        MaxPooling2D((2, 2)),\n",
        "        Flatten(),\n",
        "        #Dense(64, activation='relu'),\n",
        "        Dense(128, activation='relu', kernel_regularizer=l2(0.01)),\n",
        "        Dropout(0.5),\n",
        "        Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "    return model"
      ],
      "metadata": {
        "id": "6kltDJynjeh1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def calculate_similarity(feature1, feature2):\n",
        "    return cosine_similarity(feature1.reshape(1, -1), feature2.reshape(1, -1))[0][0]"
      ],
      "metadata": {
        "id": "ct2odwL-qMyn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "input_shape = (128, 128, 1)\n",
        "num_classes = 5\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "model = build_model(input_shape, num_classes)\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "q-i1mFexqM-S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "history = model.fit(X_train, y_train, epochs=15, validation_split=0.2)\n"
      ],
      "metadata": {
        "id": "eLxi7vASjeok",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef413245-8bd5-4927-bd1a-8d6127ca948e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "12/12 [==============================] - 11s 886ms/step - loss: 4.3674 - accuracy: 0.2396 - val_loss: 3.3762 - val_accuracy: 0.5729\n",
            "Epoch 2/15\n",
            "12/12 [==============================] - 8s 655ms/step - loss: 3.1936 - accuracy: 0.5208 - val_loss: 2.9356 - val_accuracy: 0.6875\n",
            "Epoch 3/15\n",
            "12/12 [==============================] - 10s 818ms/step - loss: 2.8909 - accuracy: 0.6536 - val_loss: 2.7751 - val_accuracy: 0.6771\n",
            "Epoch 4/15\n",
            "12/12 [==============================] - 9s 749ms/step - loss: 2.5719 - accuracy: 0.7786 - val_loss: 2.6378 - val_accuracy: 0.7500\n",
            "Epoch 5/15\n",
            "12/12 [==============================] - 8s 643ms/step - loss: 2.3315 - accuracy: 0.8542 - val_loss: 2.6599 - val_accuracy: 0.7188\n",
            "Epoch 6/15\n",
            "12/12 [==============================] - 9s 750ms/step - loss: 2.2628 - accuracy: 0.8490 - val_loss: 2.6392 - val_accuracy: 0.6562\n",
            "Epoch 7/15\n",
            "12/12 [==============================] - 9s 774ms/step - loss: 2.1528 - accuracy: 0.8542 - val_loss: 2.4510 - val_accuracy: 0.7188\n",
            "Epoch 8/15\n",
            "12/12 [==============================] - 8s 658ms/step - loss: 1.9192 - accuracy: 0.9245 - val_loss: 2.4262 - val_accuracy: 0.7396\n",
            "Epoch 9/15\n",
            "12/12 [==============================] - 9s 773ms/step - loss: 1.8034 - accuracy: 0.9297 - val_loss: 2.4668 - val_accuracy: 0.7500\n",
            "Epoch 10/15\n",
            "12/12 [==============================] - 9s 799ms/step - loss: 1.7241 - accuracy: 0.9401 - val_loss: 2.3766 - val_accuracy: 0.7708\n",
            "Epoch 11/15\n",
            "12/12 [==============================] - 8s 662ms/step - loss: 1.6347 - accuracy: 0.9479 - val_loss: 2.3675 - val_accuracy: 0.7396\n",
            "Epoch 12/15\n",
            "12/12 [==============================] - 13s 1s/step - loss: 1.5892 - accuracy: 0.9635 - val_loss: 2.2875 - val_accuracy: 0.7396\n",
            "Epoch 13/15\n",
            "12/12 [==============================] - 9s 772ms/step - loss: 1.4914 - accuracy: 0.9635 - val_loss: 2.3163 - val_accuracy: 0.7083\n",
            "Epoch 14/15\n",
            "12/12 [==============================] - 9s 758ms/step - loss: 1.4356 - accuracy: 0.9792 - val_loss: 2.2329 - val_accuracy: 0.7188\n",
            "Epoch 15/15\n",
            "12/12 [==============================] - 8s 663ms/step - loss: 1.3783 - accuracy: 0.9844 - val_loss: 2.1973 - val_accuracy: 0.7917\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1232"
      ],
      "metadata": {
        "id": "8e3gkFFeolFY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_features(model, audio_path):\n",
        "    # Extract the melspectrogram data\n",
        "    spectrograms = process_and_visualize(audio_path)\n",
        "\n",
        "    # Check if the returned list is not empty and prepare the data\n",
        "    if spectrograms:\n",
        "        # Option 1: Use the first spectrogram\n",
        "        melspectrogram = spectrograms[0]\n",
        "\n",
        "        # Option 2: Average the spectrograms (uncomment the following lines if this approach is needed)\n",
        "        # melspectrogram = np.mean(np.array(spectrograms), axis=0)\n",
        "\n",
        "        # Reshape the melspectrogram to fit the model's input expectations\n",
        "        melspectrogram = melspectrogram.reshape(1, *melspectrogram.shape)\n",
        "\n",
        "        # Use the model to predict or extract features\n",
        "        features = model.predict(melspectrogram)\n",
        "        return features\n",
        "\n",
        "    else:\n",
        "        raise ValueError(\"No spectrograms generated from the audio processing.\")\n"
      ],
      "metadata": {
        "id": "_o5ksc6Joh-Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "\n",
        "\n",
        "tflite_model = converter.convert()"
      ],
      "metadata": {
        "id": "0yyVtVp0Oz6g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "with open('model.tflite', 'wb') as f:\n",
        "    f.write(tflite_model)\n",
        "\n",
        "np.save('features.npy',category_features)"
      ],
      "metadata": {
        "id": "O2Uoa0xQO1MR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}