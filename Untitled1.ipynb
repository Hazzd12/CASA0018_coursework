{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN/3QXSY/fxeiXlBg5LjgKD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hazzd12/CASA0018_coursework/blob/main/Untitled1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Audio preprocessing\n",
        "First, we need to convert the original audio file to the Mayer spectrum, a common representation of audio features that is particularly suitable for feeding convolutional neural networks (CNNS) for training."
      ],
      "metadata": {
        "id": "MdY8G9PvjSUk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "kSJ7DXfcjDf1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import librosa\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# 假设的数据预处理函数\n",
        "def audio_to_melspectrogram(audio_path):\n",
        "    y, sr = librosa.load(audio_path, sr=None)\n",
        "    mels = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128)\n",
        "    mels_db = librosa.power_to_db(mels, ref=np.max)\n",
        "    return np.expand_dims(mels_db, axis=-1)"
      ],
      "metadata": {
        "id": "ulq-2cmvirdn"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "def unzip_audio_files(zip_path, extract_path):\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_path)\n",
        "    print(f\"Extracted audio files to {extract_path}\")\n",
        "\n",
        "# 设置ZIP文件路径和解压目录\n",
        "zip_path = '/content/data/ani.zip'\n",
        "extract_path = '/content/data/data'\n",
        "\n",
        "# 解压ZIP文件\n",
        "unzip_audio_files(zip_path, extract_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "id": "7Lye6RTL3HKP",
        "outputId": "d1d61b9e-dcb2-4850-ac5e-e22b7e266623"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'path/content/data/ani.zip'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-417cdfdccfd8>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# 解压ZIP文件\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0munzip_audio_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextract_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-3-417cdfdccfd8>\u001b[0m in \u001b[0;36munzip_audio_files\u001b[0;34m(zip_path, extract_path)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0munzip_audio_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextract_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mzip_ref\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mzip_ref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextractall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextract_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Extracted audio files to {extract_path}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/zipfile.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file, mode, compression, allowZip64, compresslevel, strict_timestamps)\u001b[0m\n\u001b[1;32m   1249\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1250\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1251\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilemode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1252\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1253\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mfilemode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodeDict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'path/content/data/ani.zip'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data_and_labels(audio_dir):\n",
        "    categories = [f.name for f in os.scandir(audio_dir) if f.is_dir()]\n",
        "    labels_dict = {category: i for i, category in enumerate(categories)}\n",
        "\n",
        "    X, y = [], []\n",
        "    for category, label in labels_dict.items():\n",
        "        category_dir = Path(audio_dir) / category\n",
        "        for audio_file in category_dir.glob('*.wav'):\n",
        "            try:\n",
        "                spectrogram = audio_to_melspectrogram(str(audio_file))\n",
        "                X.append(spectrogram)\n",
        "                y.append(label)\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {audio_file}: {e}\")\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# 加载数据和标签\n",
        "X, y = load_data_and_labels(extract_path)\n",
        "\n",
        "# 可选：保存数据和标签为NumPy数组文件\n",
        "np.save('X.npy', X)\n",
        "np.save('y.npy', y)"
      ],
      "metadata": {
        "id": "RyFfKdoAtLq7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 模型构建函数\n",
        "def build_model(input_shape, num_classes):\n",
        "    model = Sequential([\n",
        "        Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),\n",
        "        MaxPooling2D((2, 2)),\n",
        "        Conv2D(64, (3, 3), activation='relu'),\n",
        "        MaxPooling2D((2, 2)),\n",
        "        Flatten(),\n",
        "        Dense(128, activation='relu'),\n",
        "        Dropout(0.5),\n",
        "        Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "    return model"
      ],
      "metadata": {
        "id": "6kltDJynjeh1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 相似度计算函数\n",
        "def calculate_similarity(feature1, feature2):\n",
        "    return cosine_similarity(feature1.reshape(1, -1), feature2.reshape(1, -1))[0][0]"
      ],
      "metadata": {
        "id": "ct2odwL-qMyn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 数据集分割（这里使用假设的X和y）\n",
        "input_shape = (128, 128, 1)\n",
        "num_classes = 10  # 假设的类别数量\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 构建并编译模型\n",
        "model = build_model(input_shape, num_classes)\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "q-i1mFexqM-S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 训练模型\n",
        "history = model.fit(X_train, y_train, epochs=10, validation_split=0.2)\n"
      ],
      "metadata": {
        "id": "eLxi7vASjeok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def calculate_similarity(feature1, feature2):\n",
        "    similarity = cosine_similarity(feature1.reshape(1, -1), feature2.reshape(1, -1))[0][0]\n",
        "    return similarity\n",
        "\n"
      ],
      "metadata": {
        "id": "KGj-jjHqjeuY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 特征提取器\n",
        "feature_layer = model.get_layer('flatten')\n",
        "feature_extractor = Model(inputs=model.input, outputs=feature_layer.output)\n"
      ],
      "metadata": {
        "id": "TrdEXsW01DAD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 为新的音频数据提取特征（假设）\n",
        "human_audio_path = 'path/to/human_audio.wav'\n",
        "human_spectrogram = audio_to_melspectrogram(human_audio_path)\n",
        "human_feature = feature_extractor.predict(np.array([human_spectrogram]))"
      ],
      "metadata": {
        "id": "XJebdZ9C2IPB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 与特定动物特征进行比较（假设）\n",
        "animal_feature = np.random.rand(1, 128)  # 假设特征\n",
        "similarity_score = calculate_similarity(human_feature, animal_feature)\n",
        "print(f\"Similarity score: {similarity_score}\")"
      ],
      "metadata": {
        "id": "aRhm7KG62Lux"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}