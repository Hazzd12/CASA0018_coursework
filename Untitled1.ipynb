{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMAnQ7iKGRXS2wUUeqXZ5Kb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hazzd12/CASA0018_coursework/blob/main/Untitled1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Audio preprocessing\n",
        "First, we need to convert the original audio file to the Mayer spectrum, a common representation of audio features that is particularly suitable for feeding convolutional neural networks (CNNS) for training."
      ],
      "metadata": {
        "id": "MdY8G9PvjSUk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "kSJ7DXfcjDf1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import librosa\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def add_noise(audio, noise_factor=0.005):\n",
        "    noise = np.random.randn(len(audio))\n",
        "    augmented_audio = audio + noise_factor * noise\n",
        "    return augmented_audio\n",
        "\n",
        "# 假设的数据预处理函数\n",
        "def audio_to_melspectrogram(audio_path, shape=(128, 128, 1)):\n",
        "    y, sr = librosa.load(audio_path, sr=None)\n",
        "    mels = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=shape[0])\n",
        "    mels_db = librosa.power_to_db(mels, ref=np.max)\n",
        "    # 确保mels_db的大小与指定的shape相匹配\n",
        "    if mels_db.shape[1] < shape[1]:\n",
        "        pad_width = shape[1] - mels_db.shape[1]\n",
        "        mels_db = np.pad(mels_db, pad_width=((0, 0), (0, pad_width)), mode='constant')\n",
        "    else:\n",
        "        mels_db = mels_db[:, :shape[1]]\n",
        "    return np.expand_dims(mels_db, axis=-1)"
      ],
      "metadata": {
        "id": "ulq-2cmvirdn"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "def unzip_audio_files(zip_path, extract_path):\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_path)\n",
        "    print(f\"Extracted audio files to {extract_path}\")\n",
        "\n",
        "def delete_directory(directory_path):\n",
        "    try:\n",
        "        shutil.rmtree(directory_path)\n",
        "        print(f\"Directory '{directory_path}' deleted successfully.\")\n",
        "    except OSError as e:\n",
        "        print(f\"Error: {directory_path} : {e.strerror}\")\n",
        "\n",
        "# 设置ZIP文件路径和解压目录\n",
        "zip_path = '/content/dataset/Data.zip'\n",
        "extract_path = '/content/dataset/data'\n",
        "\n",
        "# 解压ZIP文件\n",
        "delete_directory(extract_path)\n",
        "unzip_audio_files(zip_path, extract_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Lye6RTL3HKP",
        "outputId": "43a51aa9-1af2-43a2-f253-5d7769a84a14"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Directory '/content/dataset/data' deleted successfully.\n",
            "Extracted audio files to /content/dataset/data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ZURszbUJAz27"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "def load_data_and_labels(audio_dir):\n",
        "    categories = [f.name for f in os.scandir(audio_dir) if f.is_dir()]\n",
        "    labels_dict = {category: i for i, category in enumerate(categories)}\n",
        "    print(labels_dict)\n",
        "    X, y = [], []\n",
        "    for category, label in labels_dict.items():\n",
        "        category_dir = Path(audio_dir) / category\n",
        "        for audio_file in category_dir.glob('*.ogg'):  # 直接处理ogg文件\n",
        "            try:\n",
        "                #audio_file = add_noise(str(audio_file))\n",
        "                spectrogram = audio_to_melspectrogram(str(audio_file))\n",
        "\n",
        "                X.append(spectrogram)\n",
        "                y.append(label)\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {audio_file}: {e}\")\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# 加载数据和标签\n",
        "X, y = load_data_and_labels(extract_path+'/Data')\n",
        "# 可选：保存数据和标签为NumPy数组文件\n",
        "np.save('X.npy', X)\n",
        "np.save('y.npy', y)\n"
      ],
      "metadata": {
        "id": "RyFfKdoAtLq7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53e01c92-a0ab-4b84-a878-885da4b3a6b5"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'105 - Frog': 0, '101 - Dog': 1, '104 - Cow': 2, '102 - Rooster': 3, '103 - Pig': 4}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 模型构建函数\n",
        "def build_model(input_shape, num_classes):\n",
        "    model = Sequential([\n",
        "        Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),\n",
        "        MaxPooling2D((2, 2)),\n",
        "        Conv2D(64, (3, 3), activation='relu'),\n",
        "        MaxPooling2D((2, 2)),\n",
        "        Flatten(),\n",
        "        Dense(128, activation='relu'),\n",
        "        Dropout(0.5),\n",
        "        Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "    return model"
      ],
      "metadata": {
        "id": "6kltDJynjeh1"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 相似度计算函数\n",
        "def calculate_similarity(feature1, feature2):\n",
        "    return cosine_similarity(feature1.reshape(1, -1), feature2.reshape(1, -1))[0][0]"
      ],
      "metadata": {
        "id": "ct2odwL-qMyn"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 数据集分割（这里使用假设的X和y）\n",
        "input_shape = (128, 128, 1)\n",
        "num_classes = 5  # 假设的类别数量\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "# 构建并编译模型\n",
        "model = build_model(input_shape, num_classes)\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "q-i1mFexqM-S"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "early_stopping_callback = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=5,  # 连续5个epoch验证集损失没有改善时停止训练\n",
        "    restore_best_weights=True\n",
        ")\n",
        "# 训练模型\n",
        "history = model.fit(X_train, y_train, epochs=20, validation_split=0.2)\n"
      ],
      "metadata": {
        "id": "eLxi7vASjeok",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44220169-911e-4f3c-9d74-09d806ae12c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "4/4 [==============================] - 6s 2s/step - loss: 0.0602 - accuracy: 0.9609 - val_loss: 1.2555 - val_accuracy: 0.7500\n",
            "Epoch 2/20\n",
            "4/4 [==============================] - 3s 770ms/step - loss: 0.0722 - accuracy: 0.9688 - val_loss: 1.0587 - val_accuracy: 0.7812\n",
            "Epoch 3/20\n",
            "4/4 [==============================] - 3s 815ms/step - loss: 0.0529 - accuracy: 0.9844 - val_loss: 0.9458 - val_accuracy: 0.7500\n",
            "Epoch 4/20\n",
            "4/4 [==============================] - 3s 771ms/step - loss: 0.0709 - accuracy: 0.9766 - val_loss: 0.9218 - val_accuracy: 0.7500\n",
            "Epoch 5/20\n",
            "4/4 [==============================] - 4s 1s/step - loss: 0.2050 - accuracy: 0.9453 - val_loss: 1.7472 - val_accuracy: 0.7188\n",
            "Epoch 6/20\n",
            "4/4 [==============================] - 3s 772ms/step - loss: 0.0802 - accuracy: 0.9688 - val_loss: 1.0776 - val_accuracy: 0.7188\n",
            "Epoch 7/20\n",
            "4/4 [==============================] - 3s 794ms/step - loss: 0.1030 - accuracy: 0.9531 - val_loss: 0.8059 - val_accuracy: 0.7188\n",
            "Epoch 8/20\n",
            "4/4 [==============================] - 3s 813ms/step - loss: 0.0792 - accuracy: 0.9844 - val_loss: 0.5927 - val_accuracy: 0.7812\n",
            "Epoch 9/20\n",
            "4/4 [==============================] - 4s 1s/step - loss: 0.0743 - accuracy: 0.9844 - val_loss: 0.6036 - val_accuracy: 0.7500\n",
            "Epoch 10/20\n",
            "4/4 [==============================] - 3s 820ms/step - loss: 0.0701 - accuracy: 0.9922 - val_loss: 0.7438 - val_accuracy: 0.7812\n",
            "Epoch 11/20\n",
            "4/4 [==============================] - 3s 767ms/step - loss: 0.0627 - accuracy: 0.9844 - val_loss: 0.5725 - val_accuracy: 0.8438\n",
            "Epoch 12/20\n",
            "4/4 [==============================] - 3s 882ms/step - loss: 0.0553 - accuracy: 0.9922 - val_loss: 0.5655 - val_accuracy: 0.8125\n",
            "Epoch 13/20\n",
            "4/4 [==============================] - 4s 886ms/step - loss: 0.0497 - accuracy: 0.9844 - val_loss: 0.9075 - val_accuracy: 0.8125\n",
            "Epoch 14/20\n",
            "4/4 [==============================] - 3s 812ms/step - loss: 0.0519 - accuracy: 0.9922 - val_loss: 0.7275 - val_accuracy: 0.7500\n",
            "Epoch 15/20\n",
            "4/4 [==============================] - 3s 783ms/step - loss: 0.0546 - accuracy: 0.9844 - val_loss: 0.5765 - val_accuracy: 0.8438\n",
            "Epoch 16/20\n",
            "4/4 [==============================] - 3s 910ms/step - loss: 0.0580 - accuracy: 0.9844 - val_loss: 0.5471 - val_accuracy: 0.7812\n",
            "Epoch 17/20\n",
            "4/4 [==============================] - 4s 926ms/step - loss: 0.0603 - accuracy: 0.9844 - val_loss: 0.4727 - val_accuracy: 0.8125\n",
            "Epoch 18/20\n",
            "4/4 [==============================] - 3s 888ms/step - loss: 0.0718 - accuracy: 0.9922 - val_loss: 0.4756 - val_accuracy: 0.8438\n",
            "Epoch 19/20\n",
            "4/4 [==============================] - 3s 830ms/step - loss: 0.0587 - accuracy: 0.9922 - val_loss: 0.4818 - val_accuracy: 0.8438\n",
            "Epoch 20/20\n",
            "4/4 [==============================] - 4s 1s/step - loss: 0.0596 - accuracy: 0.9844 - val_loss: 0.4935 - val_accuracy: 0.7812\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def calculate_similarity(feature1, feature2):\n",
        "    similarity = cosine_similarity(feature1.reshape(1, -1), feature2.reshape(1, -1))[0][0]\n",
        "    return similarity\n",
        "\n"
      ],
      "metadata": {
        "id": "KGj-jjHqjeuY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 特征提取器\n",
        "feature_layer = model.get_layer('flatten')\n",
        "feature_extractor = Model(inputs=model.input, outputs=feature_layer.output)\n"
      ],
      "metadata": {
        "id": "TrdEXsW01DAD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 为新的音频数据提取特征（假设）\n",
        "human_audio_path = 'path/to/human_audio.wav'\n",
        "human_spectrogram = audio_to_melspectrogram(human_audio_path)\n",
        "human_feature = feature_extractor.predict(np.array([human_spectrogram]))"
      ],
      "metadata": {
        "id": "XJebdZ9C2IPB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 与特定动物特征进行比较（假设）\n",
        "animal_feature = np.random.rand(1, 128)  # 假设特征\n",
        "similarity_score = calculate_similarity(human_feature, animal_feature)\n",
        "print(f\"Similarity score: {similarity_score}\")"
      ],
      "metadata": {
        "id": "aRhm7KG62Lux"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}